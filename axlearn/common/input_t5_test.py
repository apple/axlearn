# Copyright Â© 2023 Apple Inc.

"""Tests T5 inputs."""
import logging
import os
from typing import Dict, List, Tuple

import pytest
import seqio
import tensorflow as tf
from absl.testing import absltest, parameterized

from axlearn.common.config import config_for_function
from axlearn.common.input_fake import fake_source
from axlearn.common.input_t5 import (
    _noise_span_to_unique_sentinel,
    _nonnoise_span_to_unique_sentinel,
    _random_segmentation,
    apply_t5_mask,
    make_t5_autoregressive_inputs,
    map_prefix_to_value,
    random_spans_helper,
    random_spans_noise_mask,
    reduce_concat_tokens,
    select_random_chunk,
    split_tokens,
)
from axlearn.common.input_text_test import tokenizers_dir

t5_sentence_piece_vocab_file = os.path.join(tokenizers_dir, "sentencepiece/t5-base")


def _t5_vocab():
    return seqio.SentencePieceVocabulary(
        sentencepiece_model_file=t5_sentence_piece_vocab_file, extra_ids=100
    )


def _is_sentinel(vocab, token_id):
    return token_id >= (vocab.vocab_size - vocab.extra_ids)


def _count_sentinels(vocab, ids):
    return tf.reduce_sum(tf.cast(ids >= (vocab.vocab_size - vocab.extra_ids), dtype=tf.int32))


def _interleave(vocab, source_ids, target_labels):
    # Test that source_ids and target_labels are inverses of each other, by interleaving
    # source_ids and target_labels.
    interleaved = []
    t = 1  # t=0 is always a sentinel.
    for s in source_ids:
        if _is_sentinel(vocab, s):
            while t < len(target_labels) and not _is_sentinel(vocab, target_labels[t]):
                interleaved.append(target_labels[t])
                t += 1
            t += 1
        else:
            interleaved.append(s)
    logging.info("source_ids=%s", source_ids)
    logging.info("target_labels=%s", target_labels)
    logging.info("interleaved=%s", interleaved)
    return interleaved


class T5InputTest(parameterized.TestCase, tf.test.TestCase):
    def _assert_oneof(self, actual: tf.Tensor, candidates: List[List[int]]):
        for candidate in candidates:
            try:
                self.assertAllEqual(actual, candidate)
                return
            except AssertionError:
                pass
        raise ValueError(f"Expected {actual} to be equal to one of {candidates}")

    @parameterized.parameters(
        # Test a basic case where length == chunk_size.
        dict(
            chunk_size=2,
            examples=[tf.constant([1, 2])],
            expected_one_of=[[tf.constant([1, 2])]],
        ),
        # Test a basic case where length % chunk_size == 0.
        dict(
            chunk_size=2,
            examples=[tf.constant([1, 2, 3, 4])],
            expected_one_of=[[tf.constant([1, 2]), tf.constant([3, 4])]],
        ),
        # Test a basic case where length % chunk_size != 0.
        dict(
            chunk_size=3,
            examples=[tf.constant([1, 2, 3, 4, 5])],
            expected_one_of=[[tf.constant([1, 2, 3]), tf.constant([4, 5])]],
        ),
        # Test a basic case where examples are smaller than chunk_size.
        dict(
            chunk_size=4,
            examples=[tf.constant([1, 2, 3])],
            expected_one_of=[[tf.constant([1, 2, 3])]],
        ),
        # Test a basic case where examples are sometimes empty (i.e., filtered).
        dict(
            chunk_size=2,
            examples=[tf.constant([1, 2, 3]), tf.constant([]), tf.constant([4])],
            expected_one_of=[
                [tf.constant([1, 2]), tf.constant([3])],
                [tf.constant([4])],
            ],
        ),
        # Test a basic case where examples are long.
        dict(
            chunk_size=2,
            examples=[tf.constant([1, 2, 3, 4, 5])],
            expected_one_of=[[tf.constant([1, 2]), tf.constant([3, 4]), tf.constant([5])]],
        ),
    )
    def test_select_random_chunk(
        self, chunk_size: int, examples: List[tf.Tensor], expected_one_of: List[List[tf.Tensor]]
    ):
        def _check(ds):
            ds = list(ds)
            self.assertGreaterEqual(len(expected_one_of), len(ds))
            for expect, actual in zip(expected_one_of, ds):
                self._assert_oneof(actual["source_ids"].numpy().tolist(), expect)

        source = fake_source(
            is_training=False,
            examples=[{"source_ids": x} for x in examples],
            spec={"source_ids": tf.TensorSpec(shape=(None,), dtype=tf.int32)},
        )
        processor = select_random_chunk(chunk_size=chunk_size)
        _check(processor(source()))

    @parameterized.parameters(
        # Test a basic case with num_examples % batch_size == 0.
        dict(
            repeat=4,
            batch_size=2,
            examples=[tf.constant([1, 2, 3])],
            expected=[
                tf.constant([1, 2, 3] * 2),
                tf.constant([1, 2, 3] * 2),
            ],
        ),
        # Test a basic case when num_examples % batch_size != 0.
        dict(
            repeat=8,
            batch_size=3,
            examples=[tf.constant([1, 2, 3])],
            expected=[
                tf.constant([1, 2, 3] * 3),
                tf.constant([1, 2, 3] * 3),
                tf.constant([1, 2, 3] * 2),
            ],
        ),
        # Test a basic case when num_examples < batch_size.
        dict(
            repeat=2,
            batch_size=3,
            examples=[tf.constant([1, 2, 3])],
            expected=[tf.constant([1, 2, 3] * 2)],
        ),
        # Test a case when examples form a ragged.
        dict(
            batch_size=4,
            examples=[
                tf.constant([1, 2]),
                tf.constant([3, 4, 5, 6]),
                tf.constant([]),
                tf.constant([7, 8]),
                tf.constant([9]),
            ],
            expected=[
                tf.constant([1, 2, 3, 4, 5, 6, 7, 8]),
                tf.constant([9]),
            ],
        ),
        # Test a case when examples initially have padding, which gets stripped.
        dict(
            batch_size=3,
            examples=[
                tf.constant([1, 0, 2]),
                tf.constant([3, 4, 0, 0]),
                tf.constant([0]),
            ],
            expected=[tf.constant([1, 2, 3, 4])],
        ),
    )
    def test_reduce_concat_tokens(
        self, batch_size: int, examples: List[tf.Tensor], expected: List[tf.Tensor], repeat: int = 1
    ):
        def _check(ds):
            ds = list(ds)
            self.assertEqual(len(expected), len(ds))
            for expect, actual in zip(expected, ds):
                self.assertAllEqual(expect, actual["source_ids"])

        source = fake_source(
            is_training=False,
            examples=[{"source_ids": x} for x in examples],
            spec={"source_ids": tf.TensorSpec(shape=(None,), dtype=tf.int32)},
            repeat=repeat,
        )
        processor = reduce_concat_tokens(batch_size=batch_size)
        _check(processor(source()))

    @parameterized.parameters(
        dict(
            desired_source_length=10, noise_density=0.0, mean_noise_span_length=3.0, expected=[9, 1]
        ),
        dict(
            desired_source_length=10,
            noise_density=1.0,
            mean_noise_span_length=1.0,
            expected=[9, 19],
        ),
        dict(
            desired_source_length=10,
            noise_density=0.15,
            mean_noise_span_length=3.0,
            expected=[10, 4],
        ),
        dict(
            desired_source_length=10,
            noise_density=0.5,
            mean_noise_span_length=3.0,
            expected=[14, 10],
        ),
    )
    def test_random_spans_helper(
        self,
        expected: Tuple[int, int],
        desired_source_length: int,
        noise_density: float,
        mean_noise_span_length: float,
    ):
        # pylint: disable-next=missing-kwoa
        self.assertAllEqual(
            expected,
            random_spans_helper(
                desired_source_length=desired_source_length,
                noise_density=noise_density,
                mean_noise_span_length=mean_noise_span_length,
            ),
        )

    @parameterized.parameters(
        # Test when length % max_tokens_per_segment == 0.
        dict(
            examples=[tf.constant([1, 2, 3, 4])],
            max_tokens_per_segment=2,
            expected=[tf.constant([1, 2]), tf.constant([3, 4])],
        ),
        # Test when length % max_tokens_per_segment != 0.
        dict(
            examples=[tf.constant([1, 2, 3, 4, 5])],
            max_tokens_per_segment=2,
            expected=[tf.constant([1, 2]), tf.constant([3, 4]), tf.constant([5])],
        ),
        # Test when length == max_tokens_per_segment.
        dict(
            examples=[tf.constant([1, 2, 3])],
            max_tokens_per_segment=3,
            expected=[tf.constant([1, 2, 3])],
        ),
        # Test when length < max_tokens_per_segment.
        dict(
            examples=[tf.constant([1, 2, 3])],
            max_tokens_per_segment=5,
            expected=[tf.constant([1, 2, 3])],
        ),
    )
    def test_split_tokens(
        self, examples: List[tf.Tensor], max_tokens_per_segment: int, expected: List[tf.Tensor]
    ):
        def _check(ds):
            ds = list(ds)
            self.assertEqual(len(expected), len(ds))
            for expect, actual in zip(expected, ds):
                self.assertAllEqual(expect, actual["source_ids"])

        source = fake_source(
            is_training=False,
            examples=[{"source_ids": x} for x in examples],
            spec={"source_ids": tf.TensorSpec(shape=(None,), dtype=tf.int32)},
        )
        processor = split_tokens(max_tokens_per_segment=max_tokens_per_segment)
        _check(processor(source()))

    @parameterized.parameters(
        dict(seq_len=10, num_segments=3),
        dict(seq_len=9, num_segments=3),
        dict(seq_len=3, num_segments=9),
    )
    def test_random_segmentation(self, seq_len: int, num_segments: int):
        actual = _random_segmentation(seq_len, num_segments)
        self.assertEqual(sum(actual), seq_len)
        self.assertEqual(len(actual), min(seq_len, num_segments))
        self.assertFalse(tf.reduce_any(tf.equal(actual, 0)))

    @parameterized.parameters(
        # Test some basic cases.
        dict(
            noise_density=0,
            seq_len=4,
            mean_noise_span_length=2.0,
            # Example output: [False, False, False, False],
        ),
        dict(
            noise_density=0.25,
            seq_len=4,
            mean_noise_span_length=2.0,
            # Example output: [False, False, False, True],
        ),
        # Test when number of tokens to be masked is less than `mean_noise_span_length`.
        dict(
            noise_density=0.2,
            seq_len=10,
            mean_noise_span_length=3.0,
            # Example output: [False, False, False, False, False, False, False, False, True, True],
        ),
        # Test when number of tokens to be masked is not divisible by `mean_noise_span_length`.
        dict(
            noise_density=0.3,
            seq_len=10,
            mean_noise_span_length=2.0,
            # Example output: [False, False, False, False, True, True, False, False, False, True],
        ),
        # Test with length < 2.
        dict(noise_density=0.15, seq_len=1, mean_noise_span_length=2.0),
        # TODO(markblee): Revisit this test, which breaks the original.
        # This is due to possible shape mismatch when computing `interleaved_span_lengths`.
        # dict(noise_density=1.0, seq_len=4, mean_noise_span_length=2.0),
    )
    def test_random_spans_noise_mask(
        self, seq_len: int, noise_density: float, mean_noise_span_length: float
    ):
        expected_noise_tokens = min(round(seq_len * noise_density), seq_len - 1)
        expected_noise_spans = round(expected_noise_tokens / mean_noise_span_length)
        if noise_density > 0:
            expected_noise_spans = max(expected_noise_spans, 1)

        # The first token is always a non-noise token.
        if seq_len < 2:
            expected_noise_tokens = expected_noise_spans = 0

        def _count_tokens(x):
            return int(tf.reduce_sum(tf.cast(x, dtype=tf.int32)))

        def _count_spans(x):
            x = tf.concat([[0], tf.cast(x, dtype=tf.int32)], 0)
            return int(tf.reduce_sum(tf.cast(x[1:] > x[:-1], dtype=tf.int32)))

        actual = random_spans_noise_mask(
            noise_density=noise_density,
            seq_len=seq_len,
            mean_noise_span_length=mean_noise_span_length,
        )
        self.assertEqual(_count_tokens(actual), expected_noise_tokens)
        self.assertEqual(_count_spans(actual), expected_noise_spans)

    @parameterized.parameters(
        # Test an empty input.
        dict(
            input_ids=tf.constant([]),
            noise_mask=tf.constant([], dtype=tf.bool),
            expected=tf.constant([]),
        ),
        # Test everything masked.
        dict(
            input_ids=tf.constant([1, 2, 3]),
            noise_mask=tf.constant([True, True, True]),
            expected=tf.constant([32099]),
        ),
        # Test nothing masked.
        dict(
            input_ids=tf.constant([1, 2, 3]),
            noise_mask=tf.constant([False, False, False]),
            expected=tf.constant([1, 2, 3]),
        ),
        # Test some basic cases.
        dict(
            input_ids=tf.constant([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),
            noise_mask=tf.constant(
                [False, False, False, True, True, False, True, False, False, True]
            ),
            expected=tf.constant([10, 11, 12, 32099, 15, 32098, 17, 18, 32097]),
        ),
        dict(
            input_ids=tf.constant([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),
            noise_mask=tf.constant(
                [True, True, False, False, False, False, True, True, True, False]
            ),
            expected=tf.constant([32099, 12, 13, 14, 15, 32098, 19]),
        ),
    )
    @pytest.mark.skipif(
        not os.path.exists(t5_sentence_piece_vocab_file), reason="Missing testdata."
    )
    def test_noise_span_to_unique_sentinel(
        self, input_ids: tf.Tensor, noise_mask: tf.Tensor, expected: tf.Tensor
    ):
        vocab = _t5_vocab()
        actual = _noise_span_to_unique_sentinel(
            input_ids=input_ids, noise_mask=noise_mask, vocab=vocab
        )
        self.assertAllEqual(expected, actual)

    @parameterized.parameters(
        # Test an empty input.
        dict(
            input_ids=tf.constant([]),
            noise_mask=tf.constant([], dtype=tf.bool),
            expected=tf.constant([]),
        ),
        # Test everything masked.
        dict(
            input_ids=tf.constant([1, 2, 3]),
            noise_mask=tf.constant([True, True, True]),
            expected=tf.constant([1, 2, 3]),
        ),
        # Test nothing masked.
        dict(
            input_ids=tf.constant([1, 2, 3]),
            noise_mask=tf.constant([False, False, False]),
            expected=tf.constant([32099]),
        ),
        # Test some basic cases.
        dict(
            input_ids=tf.constant([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),
            noise_mask=tf.constant(
                [False, False, False, True, True, False, True, False, False, True]
            ),
            expected=tf.constant([32099, 13, 14, 32098, 16, 32097, 19]),
        ),
        dict(
            input_ids=tf.constant([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),
            noise_mask=tf.constant(
                [True, True, False, False, False, False, True, True, True, False]
            ),
            expected=tf.constant([10, 11, 32099, 16, 17, 18, 32098]),
        ),
    )
    @pytest.mark.skipif(
        not os.path.exists(t5_sentence_piece_vocab_file), reason="Missing testdata."
    )
    def test_nonnoise_span_to_unique_sentinel(
        self, input_ids: tf.Tensor, noise_mask: tf.Tensor, expected: tf.Tensor
    ):
        vocab = _t5_vocab()
        actual = _nonnoise_span_to_unique_sentinel(
            input_ids=input_ids, noise_mask=noise_mask, vocab=vocab
        )
        self.assertAllEqual(actual, expected)

    @parameterized.parameters(
        # Test an empty input.
        dict(
            source_ids=tf.constant([]),
            noise_density=0.15,
            mean_noise_span_length=3.0,
            # Example output:
            # expected=[dict(source_ids=tf.constant([]), target_labels=tf.constant([]))],
        ),
        # Test an input of length 1.
        dict(
            source_ids=tf.constant([8]),
            noise_density=0.15,
            mean_noise_span_length=3.0,
            # Example output:
            # expected=[
            #     dict(
            #         source_ids=tf.constant([8]),
            #         target_labels=tf.constant([32099]),
            #     )
            # ],
        ),
        # Test without masking.
        dict(
            source_ids=tf.constant([1, 2, 3]),
            noise_density=0.0,
            mean_noise_span_length=3.0,
            # Example output:
            # expected=[
            #     dict(
            #         source_ids=tf.constant([1, 2, 3]),
            #         target_labels=tf.constant([32099]),
            #     )
            # ],
        ),
        # Test a basic case.
        dict(
            source_ids=tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),
            noise_density=0.4,
            mean_noise_span_length=2.0,
            # Example output:
            # expected=[
            #     dict(
            #         source_ids=tf.constant([1, 2, 3, 32099, 7, 8, 9, 32098]),
            #         target_labels=tf.constant([32099, 4, 5, 6, 32098, 10]),
            #     )
            # ],
        ),
        # Test when number of tokens to be masked is less than `mean_noise_span_length`.
        dict(
            source_ids=tf.constant([1, 2]),
            noise_density=0.4,
            mean_noise_span_length=2.0,
            # Example output:
            # expected=[
            #     dict(
            #         source_ids=tf.constant([1, 32099]),
            #         target_labels=tf.constant([32099, 2]),
            #     )
            # ],
        ),
    )
    @pytest.mark.skipif(
        not os.path.exists(t5_sentence_piece_vocab_file), reason="Missing testdata."
    )
    def test_apply_t5_mask(
        self,
        source_ids: tf.Tensor,
        noise_density: float,
        mean_noise_span_length: float,
    ):
        tf.random.set_seed(1234)

        def _check(vocab, ds, source_key="source_ids", target_key="target_labels"):
            ds = list(ds)
            for example in ds:
                # Without chunking, interleaving source/target should recover original.
                actual = _interleave(vocab, example[source_key], example[target_key])
                self.assertAllEqual(source_ids, actual)
                return example

        vocab_cfg = config_for_function(_t5_vocab)
        vocab = vocab_cfg.instantiate()
        source = fake_source(
            is_training=False,
            examples=[{"source_ids": source_ids}],
            spec={"source_ids": tf.TensorSpec(shape=(None,), dtype=tf.int32)},
        )
        processor = apply_t5_mask(
            noise_density=noise_density,
            mean_noise_span_length=mean_noise_span_length,
            vocab_cfg=vocab_cfg,
        )
        _check(vocab, processor(source()))

    @parameterized.parameters(
        # Test a basic input.
        dict(
            examples=[
                dict(source_ids=tf.constant([10, 11, 12, 13, 14, 15, 16, 17])),
                dict(source_ids=tf.constant([20, 21, 22])),
                dict(source_ids=tf.constant([30, 31, 32, 33])),
                dict(source_ids=tf.constant([40, 41, 42, 43, 44, 45])),
            ],
            max_source_length=8,
            max_target_length=8,
            max_chunk_size=6,
            noise_density=0.5,
            mean_noise_span_length=3.0,
            num_sequences_to_concat=3,
            # An example output:
            # expected=[
            #     {
            #         "prefix": tf.constant([1]),
            #         "source_ids": tf.constant([10, 32099, 12, 13, 14, 15, 32098, 1]),
            #         "target_ids": tf.constant([1, 32099, 11, 32098, 20, 21, 22, 30]),
            #         "target_labels": tf.constant([32099, 11, 32098, 20, 21, 22, 30, 1]),
            #     },
            #     {
            #         "prefix": tf.constant([1]),
            #         "source_ids": tf.constant([31, 32099, 1]),
            #         "target_ids": tf.constant([1, 32099, 32, 33]),
            #         "target_labels": tf.constant([32099, 32, 33, 1]),
            #     },
            #     {
            #         "prefix": tf.constant([1]),
            #         "source_ids": tf.constant([40, 41, 42, 32099, 1]),
            #         "target_ids": tf.constant([1, 32099, 43, 44, 45]),
            #         "target_labels": tf.constant([32099, 43, 44, 45, 1]),
            #     },
            # ],
        ),
    )
    @pytest.mark.skipif(
        not os.path.exists(t5_sentence_piece_vocab_file), reason="Missing testdata."
    )
    def test_make_t5_autoregressive_inputs(self, examples: List[Dict[str, tf.Tensor]], **kwargs):
        tf.random.set_seed(1234)
        source = fake_source(
            is_training=False,
            examples=examples,
            spec={"source_ids": tf.TensorSpec(shape=(None,), dtype=tf.int32)},
        )
        vocab_cfg = config_for_function(_t5_vocab)
        processor = make_t5_autoregressive_inputs(vocab_cfg=vocab_cfg, **kwargs)
        vocab = vocab_cfg.instantiate()
        ds = list(processor(source()))

        def assert_strictly_increasing(seq):
            self.assertTrue(tf.reduce_all(tf.constant(seq[:-1]) < tf.constant(seq[1:])))

        for actual in ds:
            # Ensure that source_ids ends with EOS.
            self.assertEqual(actual["source_ids"][-1], vocab.eos_id)
            # Ensure that target_ids is a shifted version of target_labels, and starts with EOS.
            self.assertAllEqual(
                tf.concat([[vocab.eos_id], actual["target_labels"][:-1]], 0),
                actual["target_ids"],
            )
            # Ensure that target_labels ends with EOS.
            self.assertEqual(actual["target_labels"][-1], vocab.eos_id)

            source_ids = actual["source_ids"][:-1].numpy()
            target_labels = actual["target_labels"][:-1].numpy()
            # Due to how we construct our inputs, the interleaved IDs should be strictly increasing.
            assert_strictly_increasing(_interleave(vocab, source_ids, target_labels))

    @pytest.mark.skipif(
        not os.path.exists(t5_sentence_piece_vocab_file), reason="Missing testdata."
    )
    def test_make_t5_autoregressive_inputs_validation(self):
        with self.assertRaisesRegex(ValueError, "exceeds max target"):
            make_t5_autoregressive_inputs(
                vocab_cfg=config_for_function(_t5_vocab),
                max_source_length=512,
                max_target_length=1,
            )

    @parameterized.parameters(
        # Packed input.
        dict(
            is_training=True,
            example=dict(
                target=dict(
                    input_ids=tf.constant([1, 2, 3, 4, 1, 5, 6]),
                    positions=tf.constant([0, 1, 2, 3, 0, 1, 2]),
                ),
            ),
            expected=dict(
                dict(
                    target=dict(
                        input_ids=tf.constant([0, 2, 3, 4, 0, 5, 6]),
                        positions=tf.constant([0, 1, 2, 3, 0, 1, 2]),
                    ),
                ),
            ),
        ),
        dict(
            is_training=True,
            example=dict(
                dict(
                    target=dict(
                        input_ids=tf.constant([1, 1, 8, 1, 9, 10, 11]),
                        positions=tf.constant([0, 0, 1, 0, 1, 2, 3]),
                    ),
                ),
            ),
            expected=dict(
                dict(
                    target=dict(
                        input_ids=tf.constant([0, 0, 8, 0, 9, 10, 11]),
                        positions=tf.constant([0, 0, 1, 0, 1, 2, 3]),
                    ),
                ),
            ),
        ),
        # Unpacked input.
        dict(
            is_training=True,
            example=dict(
                dict(
                    target=dict(
                        input_ids=tf.constant([1, 2, 3, 4, 0, 0, 0]),
                    ),
                ),
            ),
            expected=dict(
                dict(
                    target=dict(
                        input_ids=tf.constant([0, 2, 3, 4, 0, 0, 0]),
                    ),
                ),
            ),
        ),
        # Eval input.
        dict(
            is_training=False,
            example=dict(prefix=tf.constant([1])),
            expected=dict(prefix=tf.constant([0])),
        ),
    )
    def test_map_prefix_to_value(
        self, is_training: bool, example: Dict[str, tf.Tensor], expected: Dict[str, tf.Tensor]
    ):
        source = fake_source(is_training=is_training, examples=[example])
        processor = map_prefix_to_value(is_training, value=0)
        ds = processor(source())
        actual = next(iter(ds))
        if is_training:
            tf.nest.map_structure(self.assertAllEqual, expected, actual)
        else:
            self.assertAllEqual(expected["prefix"], actual["prefix"])


if __name__ == "__main__":
    absltest.main()
