# Copyright Â© 2023 Apple Inc.

"""Fake input modules."""
import json
from typing import Any, Dict, Iterable, Optional, Sequence, Tuple, Union

import jax
import numpy as np
import tensorflow as tf

from axlearn.common.config import REQUIRED, Required, config_class
from axlearn.common.input_tf_data import BuildDatasetFn
from axlearn.common.module import Module
from axlearn.common.utils import Nested, Tensor, as_numpy_array, as_tensor


class FakeTextInput(Module):
    """Produces fake text inputs."""

    @config_class
    class Config(Module.Config):
        """Configures FakeTextInput."""

        is_training: Required[bool] = REQUIRED
        global_batch_size: Required[int] = REQUIRED  # The global batch size.
        total_num_batches: Optional[int] = None  # The total number of batches. If None, unlimited.
        source_length: int = 1024  # The length of a sequence (in tokens).
        max_token_id: int = 2048  # The maximum value a token-ID can take.

    def __init__(self, cfg: Config, *, parent=None):
        super().__init__(cfg, parent=parent)
        cfg = self.config
        if not cfg.is_training and cfg.total_num_batches is None:
            raise ValueError(f"total_num_batches should not be None if not is_training: {cfg}")
        self._prng_key = jax.random.PRNGKey(1)
        self._num_batches = 0

    def __next__(self):
        raise NotImplementedError(type(self))

    def __iter__(self):
        self._num_batches = 0
        return self

    def dataset(self):
        return self.__iter__()  # pylint: disable=unnecessary-dunder-call

    def batches(self, it: tf.data.Iterator) -> Iterable[Nested[Tensor]]:
        for input_batch in it:
            yield as_numpy_array(input_batch)


class FakeLmInput(FakeTextInput):
    """Produces fake language modeling inputs."""

    def __next__(self):
        cfg = self.config
        self._num_batches += 1
        if cfg.total_num_batches is not None and self._num_batches > cfg.total_num_batches:
            raise StopIteration()
        self._prng_key, tokens_key = jax.random.split(self._prng_key, 2)
        if cfg.global_batch_size <= 0 or cfg.global_batch_size % jax.process_count() != 0:
            raise ValueError(
                f"Global batch size ({cfg.global_batch_size}) "
                f"must be positive and divisible by process count ({jax.process_count()})"
            )
        batch_size = cfg.global_batch_size // jax.process_count()
        tokens = jax.random.randint(
            tokens_key,
            shape=[batch_size, cfg.source_length + 1],
            minval=0,
            maxval=cfg.max_token_id,
            dtype=np.int32,
        )
        return as_tensor(
            dict(
                input_ids=tokens[:, :-1],
                target_labels=tokens[:, 1:],
                target_num_bytes=jax.numpy.ones((batch_size,), dtype=jax.numpy.int32),
            )
        )


class FakeSeq2SeqInput(FakeTextInput):
    """Produces fake sequence-to-sequence inputs."""

    @config_class
    class Config(FakeTextInput.Config):
        target_length: int = 1024  # The length of a target sequence (in tokens).

    def __next__(self):
        cfg = self.config
        self._num_batches += 1
        if cfg.total_num_batches is not None and self._num_batches > cfg.total_num_batches:
            raise StopIteration()
        if cfg.global_batch_size <= 0 or cfg.global_batch_size % jax.process_count() != 0:
            raise ValueError(
                f"Global batch size ({cfg.global_batch_size}) "
                f"must be positive and divisible by process count ({jax.process_count()})"
            )
        batch_size = cfg.global_batch_size // jax.process_count()
        self._prng_key, src_tokens_key, tgt_tokens_key = jax.random.split(self._prng_key, 3)
        src_tokens = jax.random.randint(
            src_tokens_key,
            shape=[batch_size, cfg.source_length + 1],
            minval=0,
            maxval=cfg.max_token_id,
            dtype=np.int32,
        )
        tgt_tokens = jax.random.randint(
            tgt_tokens_key,
            shape=[batch_size, cfg.target_length + 1],
            minval=0,
            maxval=cfg.max_token_id,
            dtype=np.int32,
        )
        return as_tensor(
            dict(
                source_ids=src_tokens[:, :-1],
                target_ids=tgt_tokens[:, :-1],
                prefix=jax.numpy.full((batch_size, 1), -1),
                target_labels=tgt_tokens[:, 1:],
            )
        )


class FakeSequenceClassificationInput(FakeTextInput):
    """Produces fake sequence classification inputs."""

    @config_class
    class Config(FakeTextInput.Config):
        """Configures FakeSequenceClassificationInput."""

        num_labels: int = 2  # The number of different classes.

    def __next__(self):
        cfg = self.config
        self._num_batches += 1
        if cfg.total_num_batches is not None and self._num_batches > cfg.total_num_batches:
            raise StopIteration()
        self._prng_key, tokens_key = jax.random.split(self._prng_key, 2)
        if cfg.global_batch_size <= 0 or cfg.global_batch_size % jax.process_count() != 0:
            raise ValueError(
                f"Global batch size ({cfg.global_batch_size}) "
                f"must be positive and divisible by process count ({jax.process_count()})"
            )
        batch_size = cfg.global_batch_size // jax.process_count()
        input_ids = jax.random.randint(
            tokens_key,
            shape=[batch_size, cfg.source_length],
            minval=0,
            maxval=cfg.max_token_id,
            dtype=np.int32,
        )
        target_labels = jax.random.randint(
            tokens_key,
            shape=[batch_size],
            minval=0,
            maxval=cfg.num_labels,
            dtype=np.int32,
        )
        return as_tensor(
            dict(
                input_ids=input_ids,
                target_labels=target_labels,
            )
        )


class FakeExtractiveQuestionAnsweringInput(FakeTextInput):
    """Produces fake extractive QA inputs."""

    def __next__(self):
        cfg = self.config
        self._num_batches += 1
        if cfg.total_num_batches is not None and self._num_batches > cfg.total_num_batches:
            raise StopIteration()
        self._prng_key, tokens_key = jax.random.split(self._prng_key, 2)
        if cfg.global_batch_size <= 0 or cfg.global_batch_size % jax.process_count() != 0:
            raise ValueError(
                f"Global batch size ({cfg.global_batch_size}) "
                f"must be positive and divisible by process count ({jax.process_count()})"
            )
        batch_size = cfg.global_batch_size // jax.process_count()
        # TODO(@ivan-s-montero): Use fake_text_source so that tokenization will also be tested.
        input_ids = jax.random.randint(
            tokens_key,
            shape=[batch_size, cfg.source_length],
            minval=0,
            maxval=cfg.max_token_id,
            dtype=np.int32,
        )
        start_positions = jax.random.randint(
            tokens_key,
            shape=[batch_size],
            minval=0,
            maxval=cfg.source_length,
            dtype=np.int32,
        )
        end_positions = jax.random.randint(
            tokens_key,
            shape=[batch_size],
            minval=0,
            maxval=cfg.source_length,
            dtype=np.int32,
        )
        token_type_ids = (
            jax.numpy.zeros_like(input_ids).at[:, jax.numpy.min(start_positions) :].set(1)
        )
        return as_tensor(
            dict(
                input_ids=input_ids,
                start_positions=start_positions,
                end_positions=end_positions,
                token_type_ids=token_type_ids,
            )
        )


def fake_source(
    is_training: bool,
    examples: Sequence[Dict[str, tf.Tensor]],
    repeat: int = 1,
    spec: Optional[Dict[str, tf.TypeSpec]] = None,
    shuffle_buffer_size: Optional[int] = None,
) -> BuildDatasetFn:
    if len(examples) == 0:
        raise ValueError("examples cannot be empty")

    def data_gen():
        for _ in range(repeat):
            for e in examples:
                yield e

    def fn() -> tf.data.Dataset:
        ds = tf.data.Dataset.from_generator(
            data_gen,
            # If None, attempt to infer spec from elements.
            output_signature=spec or tf.nest.map_structure(tf.type_spec_from_value, examples[0]),
        )
        if is_training:
            ds = ds.repeat()
        if shuffle_buffer_size:
            if not is_training:
                raise ValueError("Shuffling should be disabled if is_training=False")
            ds = ds.shuffle(shuffle_buffer_size)
        return ds

    return fn


def fake_text_source(
    *,
    text_field_name: str = "text",
    repeat: int = 1,
    is_training: bool,
    shuffle_buffer_size: Optional[int] = None,
    batch_size: int = 2,
) -> BuildDatasetFn:
    return fake_source(
        is_training=is_training,
        examples=[
            {
                text_field_name: ("train" if is_training else "eval") + f" text {ix}",
            }
            for ix in range(2 * batch_size if is_training else batch_size)
        ],
        shuffle_buffer_size=shuffle_buffer_size,
        repeat=repeat,
    )


def fake_serialized_json_source(examples: Sequence[Dict[str, Any]]) -> BuildDatasetFn:
    """Returns a BuildDatasetFn that returns a dataset of jsonlines of examples.

    Args:
        examples: a sequence of dictionaries to dump.

    Returns:
        A BuildDatasetFn, which builds a dataset where each line is the dumped json string
        corresponding to the example.
    """

    def ds_fn() -> tf.data.Dataset:
        def data_gen():
            for ex in examples:
                yield tf.convert_to_tensor(json.dumps(ex))

        return tf.data.Dataset.from_generator(
            data_gen,
            output_signature=tf.TensorSpec(shape=[], dtype=tf.string),
        )

    return ds_fn


def fake_text2text_source(
    *,
    source_key: str = "source_text",
    target_key: str = "target_text",
    is_training: bool,
    shuffle_buffer_size: Optional[int] = None,
) -> BuildDatasetFn:
    return fake_source(
        is_training=is_training,
        examples=[
            {
                source_key: ("train" if is_training else "eval") + f" source {ix}",
                target_key: ("train" if is_training else "eval") + f" target {ix}",
            }
            for ix in range(2 * 2 if is_training else 2)
        ],
        shuffle_buffer_size=shuffle_buffer_size,
    )


def fake_glue_source(
    *,
    input_key: Union[str, Tuple[str, str]],
    label_key: str,
    is_training: bool,
    label_value: Union[int, Sequence[int]] = 0,
    num_examples: Optional[int] = None,
    shuffle_buffer_size: Optional[int] = None,
    spec: Optional[Dict[str, tf.TypeSpec]] = None,
) -> BuildDatasetFn:
    if isinstance(input_key, str):
        input_key = [input_key]
    if num_examples is None:
        num_examples = 2 * 2 if is_training else 2
    return fake_source(
        is_training=is_training,
        examples=[
            {
                # Each example has an idx and label_key, as well as all input_keys.
                "idx": ix,
                label_key: label_value,
                **{key: ("train" if is_training else "eval") + f" {key} {ix}" for key in input_key},
            }
            for ix in range(num_examples)
        ],
        shuffle_buffer_size=shuffle_buffer_size,
        spec=spec,
    )


def fake_classification_source(
    *,
    source_key: str = "source_text",
    target_key: str = "target_text",
    is_training: bool,
    classes: Sequence[str],
    shuffle_buffer_size: Optional[int] = None,
) -> BuildDatasetFn:
    num_classes = len(classes)
    return fake_source(
        is_training=is_training,
        examples=[
            {
                source_key: ("train" if is_training else "eval")
                + f" classification question {ix}, Choose {'. '.join(classes)}\n Answer:",
                target_key: f"{classes[ix % num_classes]}",
            }
            for ix in range(2 * 2 if is_training else 2)
        ],
        shuffle_buffer_size=shuffle_buffer_size,
    )


def fake_classification_source_instruct_lm(
    *,
    text_key: str = "text",
    is_training: bool,
    classes: Sequence[str] = ("yes", "no"),
    shuffle_buffer_size: Optional[int] = None,
    eoa_text: str = "<eoa>",
    eob_text: str = "<eob>",
) -> BuildDatasetFn:
    """Returns a BuildDatasetFn containing fake classification examples in the InstructLM format.

    Args:
        text_key: Data field name.
        is_training: A boolean indicating whether it is in the training mode.
        classes: A sequence of strings that specifies the class labels.
        shuffle_buffer_size: Shuffle buffer size.
        eoa_text: Text for the EOA token used in the InstructLM tokenizer.
        eob_text: Text for the EOB token used in the InstructLM tokenizer.

    Returns:
        A BuildDatasetFn containing fake classification examples with the
            "<INSTRUCTION><EOA_TOKEN><RESPONSE><EOB_TOKEN>" format.

    Note: This function is useful as evaler source input during unittest where the evaler
        metric calculator may require specific class labels as targets.
        See `evaler_generative_text_classification.py` for example.
    """
    num_classes = len(classes)
    return fake_source(
        is_training=is_training,
        examples=[
            {
                text_key: ("train" if is_training else "eval")
                + f" classification question {ix}, Choose {'. '.join(classes)}\n Answer:"
                + eoa_text
                + f" {classes[ix % num_classes]}"
                + eob_text,
            }
            for ix in range(2 * 2 if is_training else 2)
        ],
        shuffle_buffer_size=shuffle_buffer_size,
    )


def fake_speech_source(
    *,
    is_training: bool,
    num_examples: int = 100,
    speech_key: str = "speech",
    shuffle_buffer_size: Optional[int] = None,
) -> BuildDatasetFn:
    """Fake speech data source.

    Args:
        is_training: A boolean indicating whether it is in the training mode.
        num_examples: Integer of number of examples in the dataset.
        shuffle_buffer_size: Shuffle buffer size used for training.

    Returns:
        A BuildDatasetFn producing fake examples with `speech_key` with random integer values
        (assuming 16 bit-depth).
    """
    return fake_source(
        is_training=is_training,
        examples=[
            {
                speech_key: jax.random.randint(
                    jax.random.PRNGKey(ix),
                    minval=-(2**15),
                    maxval=2**15,
                    shape=[ix % 100 + 1],
                ),
            }
            for ix in range(num_examples)
        ],
        shuffle_buffer_size=shuffle_buffer_size,
        spec={speech_key: tf.TensorSpec(shape=(None,), dtype=tf.int16)},
    )
