# Copyright Â© 2025 Apple Inc.

"""Base KVCache layer."""

from typing import NamedTuple, Optional

import jax.numpy as jnp

from axlearn.common.base_layer import BaseLayer
from axlearn.common.config import config_class
from axlearn.common.utils import Nested, Tensor


class KVState(NamedTuple):
    """Represents key/value projections.

    Fields:
        k_proj: [batch, source_length, num_kv_heads, per_head_dim], Projected key tensor.
        v_proj: [batch, source_length, num_kv_heads, per_head_dim], Projected value tensor.
        key_positions: [batch, source_length], Positions of the keys in the batch.
    """

    k_proj: Tensor
    v_proj: Tensor
    key_positions: Tensor


class BaseKVCache(BaseLayer):
    """Abstract base class for KV cache."""

    @config_class
    class Config(BaseLayer.Config):
        """Configures BaseKVCache."""

        # Autoregressive KV cache dtype, which the input KV is converted into.
        # This dtype is not only used for storing the KV cache, but it also applies to
        # the returned KV activations. Therefore, before computing attention, we need to cast
        # the KV tensors to match the query's dtype.
        cache_dtype: Optional[jnp.dtype] = None

    class Output(KVState):
        pass

    class Shape(NamedTuple):
        """Shape of BaseKVCache.

        Fields:
            batch_size: Batch size.
            kv_len: KV Cache size.
            num_kv_heads: The number of KV heads.
            per_head_dim: The dimension per head.
        """

        batch_size: int
        kv_len: int
        num_kv_heads: int
        per_head_dim: int

    def _cache_dtype(self, dtype: jnp.dtype):
        # Default to activation dtype for initialization if cache_dtype is None.
        dtype = self.config.cache_dtype or dtype
        assert dtype is not None
        return dtype

    def init_states(self, shape: Shape, *, dtype: jnp.dtype) -> Nested[Tensor]:
        """Initializes KV cache.

        Args:
            shape: Shape, [batch, kv_len, num_kv_heads, per_head_dim].
            dtype: dtype for KV Cache.

        Returns:
            init_state: A `Nested[Tensor]` object containing KV cache such as key and value.
        """
        raise NotImplementedError(type(self))

    def extend_step(
        self,
        cached_states: Nested[Tensor],
        *,
        k_proj: Tensor,
        v_proj: Tensor,
        key_positions: Tensor,
        live_step_len: Optional[Tensor] = None,
    ) -> tuple[Nested[Tensor], Output]:
        """Updates the KV cache per extend step.

        The input k_proj/v_proj are generated by `i_proj.forward()` from the input tokens.
        The output k_proj/v_proj will have the `kv_cache` concatenated and will be used
        in the current attention operation.

        Args:
            cached_states: A `Nested[Tensor]` object containing KV cache such as key and value.
            k_proj: A Tensor of shape [batch, step_length, num_kv_heads, per_head_dim].
            v_proj: A Tensor of shape [batch, step_length, num_kv_heads, per_head_dim].
            key_positions: An optional Tensor of shape [1|batch, step_length].
            live_step_len: An optional Tensor of shape [batch]. Please refer to ``On live_step_len``
                in the file docstring for details.

        Returns:
            A tuple (updated_state, output):
            * updated_state: A `Nested[Tensor]` object containing KV cache such as key and value.
            * output: The output k_proj, v_proj, and key_positions, which are merged with
                KV cache, resulting in a length of `source_length`.
        """
        raise NotImplementedError(type(self))
