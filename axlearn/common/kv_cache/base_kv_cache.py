# Copyright Â© 2025 Apple Inc.

"""Base KVCache layer."""

from typing import NamedTuple, Optional

import jax.numpy as jnp

from axlearn.common.base_layer import BaseLayer
from axlearn.common.config import config_class
from axlearn.common.utils import Nested, Tensor


class KVState(NamedTuple):
    """Represents key/value projections.

    Fields:
        k_proj: [batch, source_length, num_kv_heads, per_head_dim], Projected key tensor.
        v_proj: [batch, source_length, num_kv_heads, per_head_dim], Projected value tensor.
        key_positions: [batch, source_length], Positions of the keys in the batch.
        page_indices: [batch, max_pages_per_request], optional page indices for batched requests.
    """

    k_proj: Tensor
    v_proj: Tensor
    key_positions: Tensor
    page_indices: Optional[Tensor] = None


class BaseKVCache(BaseLayer):
    """Abstract base class for KV cache."""

    @config_class
    class Config(BaseLayer.Config):
        """Configures BaseKVCache."""

        # Autoregressive KV cache dtype, which the input KV is converted into.
        # This dtype is not only used for storing the KV cache, but it also applies to
        # the returned KV activations. Therefore, before computing attention, we need to cast
        # the KV tensors to match the query's dtype.
        cache_dtype: Optional[jnp.dtype] = None

    class Output(KVState):
        pass

    class Shape(NamedTuple):
        """Shape of BaseKVCache.

        Fields:
            batch_size: Batch size.
            kv_len: KV Cache size.
            num_kv_heads: The number of KV heads.
            per_head_dim: The dimension per head.
        """

        batch_size: int
        kv_len: int
        num_kv_heads: int
        per_head_dim: int

    def _cache_dtype(self, dtype: jnp.dtype):
        # Default to activation dtype for initialization if cache_dtype is None.
        dtype = self.config.cache_dtype or dtype
        assert dtype is not None
        return dtype

    def init_states(self, shape: Shape, *, dtype: jnp.dtype) -> Nested[Tensor]:
        """Initializes KV cache.

        Args:
            shape: Shape, [batch, kv_len, num_kv_heads, per_head_dim].
            dtype: dtype for KV Cache.

        Returns:
            init_state: A `Nested[Tensor]` object containing KV cache such as key and value.
        """
        raise NotImplementedError(type(self))

    def extend_step(
        self,
        cached_states: Nested[Tensor],
        *,
        k_proj: Tensor,
        v_proj: Tensor,
        key_positions: Tensor,
        unpadded_len: Optional[Tensor] = None,
        page_pool: Optional[Nested[Tensor]] = None,
    ) -> tuple[Nested[Tensor], Output]:
        """Updates the KV cache per extend step.

        The input k_proj/v_proj are generated by `i_proj.forward()` from the input tokens.
        The output k_proj/v_proj will have the `kv_cache` concatenated and will be used
        in the current attention operation.

        Args:
            cached_states: A `Nested[Tensor]` object containing KV cache such as key and value.
            k_proj: A Tensor of shape [batch, step_length, num_kv_heads, per_head_dim].
            v_proj: A Tensor of shape [batch, step_length, num_kv_heads, per_head_dim].
            key_positions: An optional Tensor of shape [1|batch, step_length].
            unpadded_len: An optional Tensor of shape [batch]. Specifies the number of
                non-padding tokens per sequence. When provided, only the first `unpadded_len[i]`
                tokens of sequence `i` are considered valid for caching. The actual behavior
                depends on the specific KV cache implementation.
            page_pool: See file-level docstring of `attention.py`.

        Returns:
            A tuple (updated_state, output):
            * updated_state: A `Nested[Tensor]` object containing KV cache such as key and value.
            * output: The output k_proj, v_proj, and key_positions, which are merged with
                KV cache, resulting in a length of `source_length`.
        """
        raise NotImplementedError(type(self))

    @classmethod
    def maybe_normalize_kv(cls, kv_state: KVState) -> tuple[Tensor, Tensor]:
        """Normalize the KV shape if they're not already normalized.

        Returns:
            A tuple of [k_proj, v_proj], each with shape [batch_size, seq_len, kv_heads, head_dim].
        """
        assert kv_state.page_indices is None
        return kv_state.k_proj, kv_state.v_proj
