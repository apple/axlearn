# Copyright Â© 2023 Apple Inc.

"""Tests retrieval evaluation pipeline."""
# pylint: disable=no-self-use
import tempfile
from typing import Dict, List, Union

import jax
import jax.numpy as jnp
import numpy as np
from absl.testing import parameterized
from jax.experimental.pjit import pjit

from axlearn.common.attention import NEG_INF
from axlearn.common.eval_retrieval import (
    CLIPRetrievalMetricCalculator,
    CxcImageRetrievalMetricCalculator,
    EmbeddingRetrievalMetricCalculator,
    KnnMetricCalculator,
    clip_generate_labels,
    get_pairwise_distances_sqr,
)
from axlearn.common.evaler_test import DummyModel
from axlearn.common.test_utils import TestCase, assert_allclose
from axlearn.common.utils import NestedTensor, Tensor


def dummy_data_generator(
    valid_sentences: Tensor,
    image_embeddings: Tensor,
    text_embeddings: Tensor,
    batch_size: int = 2,
):
    for batch_idx in range(int(np.ceil(len(image_embeddings) / batch_size))):
        start, end = batch_idx * batch_size, (batch_idx + 1) * batch_size
        yield {
            "input": {
                "valid_sentences": jnp.asarray(valid_sentences[start:end]),
            },
            "predicted": {
                "visual_encoder": {"output_features": jnp.asarray(image_embeddings[start:end])},
                "textual_encoder": {"output_features": jnp.asarray(text_embeddings[start:end])},
            },
        }


# pylint: disable-next=abstract-method
class DummyRetrievalModel(DummyModel):
    """A dummy model whose `embed_{image,text}_batch` returns input_batch["emb"])."""

    # pylint: disable-next=no-self-use
    def predict(self, input_batch: NestedTensor) -> Tensor:
        return input_batch["predicted"]


def _compute_metrics(
    *,
    data_generator,
    calculator_cfg,
) -> Dict:
    """Computes zero-shot classification metrics.

    Args:
        text_labels: A list of class labels.
        text_embeddings: A list of embeddings, each corresponding to a label in `text_labels`.
            `text_labels` and `text_embeddings` must have the same length.
        image_labels: A list of image labels.
        image_embeddings: A list of embeddings, each corresponding to a label in `image_labels`.
            `image_labels` and `image_embeddings` must have the same length.
        top_ks: a list of K's to compute top-k accuracy.

    Returns:
        A dict containing:
        "class_embeddings": a Tensor of shape [num_classes, emb_dim] computed from text_labels
            and text_embeddings.
        "summaries": a Dict of WeightedScalar values computed by
            ZeroShotImageClassificationMetricCalculator.
    """
    with jax.sharding.Mesh(
        jax.experimental.mesh_utils.create_device_mesh((1, 1)), ("data", "model")
    ):
        model = DummyRetrievalModel.default_config().set(name="model").instantiate(parent=None)
        model_param_partition_specs = jax.tree_util.tree_map(
            lambda spec: spec.mesh_axes, model.create_parameter_specs_recursively()
        )
        calculator = calculator_cfg.set(name="calculator").instantiate(
            parent=None, model=model, model_param_partition_specs=model_param_partition_specs
        )
        model_params = pjit(
            model.initialize_parameters_recursively,
            in_shardings=(None,),
            out_shardings=model_param_partition_specs,
        )(jax.random.PRNGKey(0))

        state = calculator.init_state(prng_key=jax.random.PRNGKey(0), model_params=model_params)
        all_forward_outputs = []
        for input_batch in data_generator:
            forward_outputs = calculator.forward(
                input_batch, model_params=model_params, state=state
            )
            state = forward_outputs["state"]
            all_forward_outputs.append(forward_outputs["output"])
        summaries = calculator.get_summaries(
            model_params=model_params, state=state, all_forward_outputs=all_forward_outputs
        )
        return summaries


class CLIPRetrievalMetricCalculatorTest(TestCase, parameterized.TestCase):
    @parameterized.parameters(
        {"top_k": 1, "expected_metrics": {"i2t_top@1": 1 / 3, "t2i_top@1": 3 / 10}},
        {"top_k": 2, "expected_metrics": {"i2t_top@2": 2 / 3, "t2i_top@2": 8 / 10}},
        {
            "top_k": [1, 2],
            "expected_metrics": {
                "i2t_top@1": 1 / 3,
                "t2i_top@1": 3 / 10,
                "i2t_top@2": 2 / 3,
                "t2i_top@2": 8 / 10,
            },
        },
    )
    def test_clip_retrieval_metric_calculator(
        self, top_k: Union[int, List[int]], expected_metrics: Dict[str, float]
    ):
        text_embeddings = jnp.asarray(
            [
                [[1, 0, 0], [0, 1, 0], [0, 0, 1], [0.1, 0.1, 0.2], [100, 100, 100]],
                [[0.5, 0, 0], [0, 0.5, 0], [0, 0.5, 0.7], [100, 100, 100], [100, 100, 100]],
                [[0.6, 0, 0], [0, 0.7, 0], [0, 0.6, 1], [100, 100, 100], [100, 100, 100]],
                [
                    [100, 100, 100],
                    [100, 100, 100],
                    [100, 100, 100],
                    [100, 100, 100],
                    [100, 100, 100],
                ],
            ]
        )
        valid_sentences = [[1, 1, 1, 1, 0], [1, 1, 1, 0, 0], [1, 1, 1, 0, 0], [0, 0, 0, 0, 0]]
        image_embeddings = [
            [[1, 0, 0]],
            [[0, 1, 0]],
            [[0, 0, 1]],
            [[100, 100, 100]],
        ]
        # For Image to Text Retrieval:
        # Top_1 accuracy:
        # Image_0 retrieved Text_0 (matched).
        # Image_1 retrieved Text_1 (missed). Image_1 needs to retrieval Text_[5-9].
        # Image_2 retrieved Text_2 (missed). Image_2 needs to retrieval Text_[10-14].
        # Image_3 is padded.
        # i2t_top@1 = 1/3. Noted Image_3 is padded. Therefore it doens't count.
        #
        # Top_2 accuracy:
        # Image_0 retrieved Text_0 and Text_10 (matched).
        # Image_1 retrieved Text_1 and Text_11 (missed).
        # Image_2 retrieved Text_2 and Text_13 (matched).
        # Image_3 is padded. Therefore, it doesn't count.
        # i2t_top@2 = 2/3

        # For Text to Image Retrieval:
        # Top_1 accuracy:
        # Text_0 retrieved Image_0 (matched).
        # Text_1 retrieved Image_1 (missed).
        # Text_2 retrieved Image_2 (missed).
        # Text_3 retrieved Image_2 (missed).
        # Text_5 retrieved Image_0 (missed).
        # Text_6 retrieved Image_1 (matched).
        # Text_7 retrieved Image_2 (missed).
        # Text_10 retrieved Image_0 (missed).
        # Text_11 retrieved Image_1 (missed).
        # Text_12 retrieved Image_2 (matched).
        # t2i_top@1 = 3/10
        #
        # Top_2 accuracy:
        # Text_0 retrieved Image_0 and Image_1 (matched).
        # Text_1 retrieved Image_1 and Image_0 (matched) (tie-breaker: prefers first image).
        # Text_2 retrieved Image_2 and Image_0 (matched) (tie-breaker).
        # Text_3 retrieved Image_2 and Image_0 (matched) (tie-breaker).
        # Text_5 retrieved Image_0 and Image_1 (matched) (tie-breaker).
        # Text_6 retrieved Image_1 and Image_0 (matched).
        # Text_7 retrieved Image_2 and Image_1 (matched) (tie-breaker).
        # Text_10 retrieved Image_0 and Image_1 (missed).
        # Text_11 retrieved Image_1 and Image_0 (missed).
        # Text_12 retrieved Image_2 and Image_1 (matched).
        # t2i_top@2 = 8/10

        if isinstance(top_k, int):
            top_k = [top_k]

        summaries = _compute_metrics(
            calculator_cfg=CLIPRetrievalMetricCalculator.default_config().set(top_ks=top_k),
            data_generator=dummy_data_generator(
                valid_sentences=valid_sentences,
                image_embeddings=image_embeddings,
                text_embeddings=text_embeddings,
            ),
        )
        self.assertNestedAllClose(summaries, expected_metrics)

    def test_clip_generate_labels(self):  # pylint: disable=no-self-use
        sentence_paddings = jnp.array([[0, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 1], [1, 1, 1, 1]])
        image_paddings = jnp.array([[0], [0], [0], [1]])
        labels_dict = clip_generate_labels(sentence_paddings)
        assert_allclose(
            labels_dict["image_to_text_label"],
            [[0, 1, 2, 3], [4, 5, 6, -1], [8, 9, -1, -1], [-1, -1, -1, -1]],
        )
        assert_allclose(
            labels_dict["text_to_image_label"],
            [[0], [0], [0], [0], [1], [1], [1], [-1], [2], [2], [-1], [-1], [-1], [-1], [-1], [-1]],
        )
        image_to_text_similarity_bias = np.array(labels_dict["image_to_text_similarity_bias"])
        text_to_image_similarity_bias = np.array(labels_dict["text_to_image_similarity_bias"])
        assert_allclose(image_to_text_similarity_bias, sentence_paddings * NEG_INF)
        assert_allclose(text_to_image_similarity_bias, image_paddings * NEG_INF)


class EmbeddingRetrievalMetricCalculatorTest(TestCase, parameterized.TestCase):
    @parameterized.parameters([1, 2, 3, 100])
    def test_compute_metrics_separate_query_index(self, max_query_chunk_size):
        def data_generator(batch_size=2):
            embedding = jnp.array(
                [[1.0, 0, 0], [0, 1, 0], [0, 1, 0], [0.5, 0.5, 0.5], [1, 0, 0], [0, 1, 0]]
            )
            is_query = jnp.array([True, True, True, False, False, False])
            label = jnp.array([1, 2, 2, 2, 1, 1])
            category = jnp.array([0, 1, 0, 0, 1, 0])

            for start in range(0, embedding.shape[0], batch_size):
                end = start + batch_size
                yield {
                    "input": {
                        "is_query": is_query[start:end],
                        "label": label[start:end],
                        "category": category[start:end],
                    },
                    "predicted": {"embedding": embedding[start:end]},
                }

        calculator_cfg = EmbeddingRetrievalMetricCalculator.default_config().set(
            metrics=["MAP@1", "MAP@2", "accuracy@1", "recall@1"],
            categories_names=("cat", "dog"),
            max_query_chunk_size=max_query_chunk_size,
        )
        summaries = _compute_metrics(
            calculator_cfg=calculator_cfg,
            data_generator=data_generator(),
        )
        expected_metrics = {
            "MAP@1": (1.0 + 0.0 + 0.0) / 3,
            "MAP@1_cat": (1.0 + 0.0) / 2,
            "MAP@1_dog": 0.0,
            "MAP@1_avg_category": ((1.0 + 0.0) / 2 + 0.0) / 2,
            "MAP@2": (0.5 + 0.5 + 0.5) / 3,
            "MAP@2_cat": (0.5 + 0.5) / 2,
            "MAP@2_dog": 0.5,
            "MAP@2_avg_category": ((0.5 + 0.5) / 2 + 0.5) / 2,
            "accuracy@1": (1.0 + 0.0 + 0.0) / 3,
            "accuracy@1_cat": (1.0 + 0.0) / 2,
            "accuracy@1_dog": 0.0,
            "accuracy@1_avg_category": ((1.0 + 0.0) / 2 + 0.0) / 2,
            "recall@1": (1.0 + 0.0 + 0.0) / 3,
            "recall@1_cat": (1.0 + 0.0) / 2,
            "recall@1_dog": 0.0,
            "recall@1_avg_category": ((1.0 + 0.0) / 2 + 0.0) / 2,
            "num_valid": 3,
        }
        self.assertNestedAllClose(summaries, expected_metrics)

    @parameterized.parameters([1, 2, 3, 100])
    def test_compute_metrics_same_query_index(self, max_query_chunk_size):
        def data_generator(batch_size=2):
            embedding = jnp.array(
                [[1.0, 0, 0], [0, 1, 0], [0, 1, 0], [0.5, 0.5, 0.5], [1, 0, 0], [0, 1, 0]]
            )
            label = jnp.array([1, 2, 2, 2, 1, 1])
            category = jnp.array([0, 1, 0, 0, 1, 0])

            for start in range(0, embedding.shape[0], batch_size):
                end = start + batch_size
                yield {
                    "input": {
                        "label": label[start:end],
                        "category": category[start:end],
                    },
                    "predicted": {"embedding": embedding[start:end]},
                }

        calculator_cfg = EmbeddingRetrievalMetricCalculator.default_config().set(
            metrics=["MAP@1", "MAP@2"],
            categories_names=("cat", "dog"),
            max_query_chunk_size=max_query_chunk_size,
        )
        summaries = _compute_metrics(
            calculator_cfg=calculator_cfg,
            data_generator=data_generator(),
        )
        expected_metrics = {
            "MAP@1": (1.0 + 1.0 + 1.0 + 0.0 + 1.0 + 0.0) / 6,
            "MAP@1_cat": (1.0 + 1.0 + 0.0 + 0.0) / 4,
            "MAP@1_dog": (1.0 + 1.0) / 2,
            "MAP@1_avg_category": ((1.0 + 1.0 + 0.0 + 0.0) / 4 + (1.0 + 1.0) / 2) / 2,
            "MAP@2": (0.5 + 0.5 + 0.5 + 0.25 + 0.5 + 0.0) / 6,
            "MAP@2_cat": (0.5 + 0.5 + 0.25 + 0.0) / 4,
            "MAP@2_dog": (0.5 + 0.5) / 2,
            "MAP@2_avg_category": ((0.5 + 0.5 + 0.25 + 0.0) / 4 + (0.5 + 0.5) / 2) / 2,
            "num_valid": 6,
        }
        self.assertNestedAllClose(summaries, expected_metrics)

    def test_compute_metrics_with_prefix(self):
        def data_generator(batch_size=2):
            embedding = jnp.array(
                [[1.0, 0, 0], [0, 1, 0], [0, 1, 0], [0.5, 0.5, 0.5], [1, 0, 0], [0, 1, 0]]
            )
            label = jnp.array([1, 2, 2, 2, 1, 1])
            category = jnp.array([0, 1, 0, 0, 1, 0])

            for start in range(0, embedding.shape[0], batch_size):
                end = start + batch_size
                yield {
                    "input": {
                        "label": label[start:end],
                        "category": category[start:end],
                    },
                    "predicted": {"embedding": embedding[start:end]},
                }

        calculator_cfg = EmbeddingRetrievalMetricCalculator.default_config().set(
            metrics=["MAP@1"], categories_names=("cat", "dog"), prefix="test"
        )
        summaries = _compute_metrics(
            calculator_cfg=calculator_cfg,
            data_generator=data_generator(),
        )
        expected_metrics = {
            "test/MAP@1": (1.0 + 1.0 + 1.0 + 0.0 + 1.0 + 0.0) / 6,
            "test/MAP@1_cat": (1.0 + 1.0 + 0.0 + 0.0) / 4,
            "test/MAP@1_dog": (1.0 + 1.0) / 2,
            "test/MAP@1_avg_category": ((1.0 + 1.0 + 0.0 + 0.0) / 4 + (1.0 + 1.0) / 2) / 2,
            "test/num_valid": 6,
        }
        self.assertNestedAllClose(summaries, expected_metrics)

    def test_get_pairwise_distances_sqr(self):
        prng = jax.random.split(jax.random.PRNGKey(777))
        a = jax.random.uniform(prng[0], (16, 32))
        b = jax.random.uniform(prng[1], (64, 32))
        dists = get_pairwise_distances_sqr(a, b)
        expected_dists = ((a[:, None, :] - b[None, :, :]) ** 2).sum(axis=-1)
        self.assertNestedAllClose(expected_dists, dists)


_TEST_IMAGE_RELEVANCE_CSV = """image1,image2,agg_score,sampling_method
COCO_val2014_000000205866.jpg,COCO_val2014_000000039081.jpg,3.6,i2i_csim
COCO_val2014_000000520871.jpg,COCO_val2014_000000358039.jpg,3.99,i2i_csim
COCO_val2014_000000281782.jpg,COCO_val2014_000000447314.jpg,3.9,i2i_csim
COCO_val2014_000000529981.jpg,COCO_val2014_000000074460.jpg,3.61,i2i_csim
COCO_val2014_000000001682.jpg,COCO_val2014_000000409088.jpg,1.72,i2i_csim
"""


class CxcImageRetrievalMetricCalculatorTest(TestCase, parameterized.TestCase):
    @parameterized.parameters([1, 2, 3, 100])
    def test_compute_metrics_same_query_index(self, max_query_chunk_size):
        def data_generator(batch_size=3):
            embedding = jnp.array([[1.0, 0.0], [2.0, 0.0], [0.5, 0.5]])
            # Only 205866 and 39081 are relevant.
            image_id = jnp.array([205866, 39081, 1682])

            for start in range(0, embedding.shape[0], batch_size):
                end = start + batch_size
                yield {
                    "input": {
                        "image_id": image_id[start:end],
                    },
                    "predicted": {"embedding": embedding[start:end]},
                }

        with tempfile.NamedTemporaryFile(mode="w", suffix=".csv") as f:
            f.write(_TEST_IMAGE_RELEVANCE_CSV)
            f.flush()
            calculator_cfg = CxcImageRetrievalMetricCalculator.default_config().set(
                image_relevance_path=f.name,
                metrics=["accuracy@1", "accuracy@2"],
                max_query_chunk_size=max_query_chunk_size,
            )
            summaries = _compute_metrics(
                calculator_cfg=calculator_cfg,
                data_generator=data_generator(),
            )

        expected_metrics = {
            "accuracy@1": (0.0 + 1.0 + 0.0) / 3,
            "accuracy@2": (1.0 + 1.0 + 0.0) / 3,
            "num_valid": 3,
        }
        self.assertNestedAllClose(summaries, expected_metrics)


class KnnMetricCalculatorTest(TestCase, parameterized.TestCase):
    def test_compute_metrics(self):
        def data_generator(batch_size=2):
            embedding = jnp.array(
                [[1.0, 0, 0], [0, 1, 0], [0, 1, 0], [0.5, 0.5, 0.5], [1, 0, 0], [0, 1, 0]]
            )
            is_query = jnp.array([True, True, True, False, False, False])
            label = jnp.array([1, 1, 2, 1, 1, 2])
            category = jnp.array([0, 1, 0, 0, 1, 0])

            for start in range(0, embedding.shape[0], batch_size):
                end = start + batch_size
                yield {
                    "input": {
                        "is_query": is_query[start:end],
                        "label": label[start:end],
                        "category": category[start:end],
                    },
                    "predicted": {"embedding": embedding[start:end]},
                }

        calculator_cfg = KnnMetricCalculator.default_config().set(
            categories_names=("cat", "dog"),
            num_labels=3,
            top_ks=(1, 2, 3),
            temps=[2.01],
        )
        summaries = _compute_metrics(
            calculator_cfg=calculator_cfg,
            data_generator=data_generator(),
        )
        expected_metrics = {
            "knn-t2.01@1": (1.0 + 0.0 + 1.0) / 3.0,
            "knn-t2.01@1_cat": (1.0 + 1.0) / 2,
            "knn-t2.01@1_dog": 0.0,
            "knn-t2.01@1_avg_category": 0.5,
            "knn-t2.01@2": (1.0 + 0.0 + 1.0) / 3.0,
            "knn-t2.01@2_cat": (1.0 + 1.0) / 2,
            "knn-t2.01@2_dog": 0.0,
            "knn-t2.01@2_avg_category": 0.5,
            "knn-t2.01@3": (1.0 + 1.0 + 0.0) / 3.0,
            "knn-t2.01@3_cat": (1.0 + 0.0) / 2,
            "knn-t2.01@3_dog": 1.0,
            "knn-t2.01@3_avg_category": 0.75,
            "num_valid": 3,
        }
        self.assertNestedAllClose(summaries, expected_metrics)
