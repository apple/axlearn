# Copyright Â© 2023 Apple Inc.

"""Tests repeat layer."""
import itertools
from typing import Dict, Optional

import jax.random
from absl import logging
from absl.testing import absltest, parameterized
from jax import numpy as jnp

from axlearn.common import param_init
from axlearn.common.base_layer import BaseLayer, ParameterSpec, RematSpec
from axlearn.common.config import REQUIRED, Required, config_class, config_for_function
from axlearn.common.layers import RedirectToSharedModule
from axlearn.common.layers_test import ParentLayer
from axlearn.common.module import Module
from axlearn.common.module import functional as F
from axlearn.common.repeat import Repeat, _drop_by_regex
from axlearn.common.test_utils import TestCase, assert_allclose
from axlearn.common.utils import PartitionSpec, VDict, get_recursively, shapes


class TestLayer(BaseLayer):
    """A dummy layer."""

    def _create_layer_parameter_specs(self) -> Dict[str, ParameterSpec]:
        return dict(
            inc=ParameterSpec(
                shape=[], mesh_axes=[], initializer=param_init.constant_initializer(1)
            )
        )

    def init_forward_state(self, batch_size):
        return jnp.zeros([batch_size], dtype=self.dtype())

    def forward(self, *, carry, forward_state):
        logging.info("TestLayer: carry=%s forward_state=%s", shapes(carry), shapes(forward_state))
        forward_state = forward_state + carry
        self.add_summary("carry_mean", jnp.mean(carry))
        self.add_module_output("state", forward_state)
        return carry + self.parameters["inc"], forward_state


class TestComplicatedLayer(BaseLayer):
    """A dummy layer with children."""

    @config_class
    class Config(BaseLayer.Config):
        layer1: TestLayer.Config = TestLayer.default_config()
        layer2: TestLayer.Config = TestLayer.default_config()

    def __init__(self, cfg: Config, *, parent: Optional[Module]):
        super().__init__(cfg, parent=parent)
        cfg = self.config
        self._add_child("layer1", cfg.layer1)
        self._add_child("layer2", cfg.layer2)

    def init_forward_state(self, batch_size):
        return jnp.zeros([batch_size], dtype=self.dtype())

    def forward(self, *, carry, forward_state):
        carry, forward_state = self.layer1(carry=carry, forward_state=forward_state)
        carry, forward_state = self.layer2(carry=carry, forward_state=forward_state)
        return carry, forward_state


class TestRepeat(Repeat):
    """A dummy repeat layer."""

    @classmethod
    def default_config(cls):
        cfg = super().default_config()
        cfg.layer = TestLayer.default_config()
        return cfg

    def init_forward_state(self, batch_size):
        cfg = self.config
        layer_state = self.layer.init_forward_state(batch_size)
        return dict(
            layer=jax.tree_util.tree_map(
                lambda x: jnp.tile(x[None, :], [cfg.num_layers, 1]),
                layer_state,
            )
        )

    def forward(self, *, carry, forward_state):
        def fn(carry, forward_state_tn):
            return self.layer(carry=carry, forward_state=forward_state_tn["layer"])

        carry, forward_state = self._run(fn, carry, xs=forward_state)
        return carry, dict(layer=forward_state)


class TestEnsemble(BaseLayer):
    """A dummy ensemble layer."""

    @config_class
    class Config(BaseLayer.Config):
        num_layers: Required[int] = REQUIRED
        dummy_layer: TestLayer.Config = TestLayer.default_config()
        repeat_layer: TestRepeat.Config = TestRepeat.default_config()

    def __init__(self, cfg: Config, *, parent: Optional[Module]):
        super().__init__(cfg, parent=parent)
        cfg = self.config
        self._add_child("dummy_layer", cfg.dummy_layer)
        self._share_with_descendants(
            self.dummy_layer,
            shared_module_name="shared_layer",
        )
        self._add_child("repeat_layer", cfg.repeat_layer.set(num_layers=cfg.num_layers))

    def init_forward_state(self, batch_size):
        layer_state = self.repeat_layer.init_forward_state(batch_size)
        return dict(repeat_layer=layer_state)

    def forward(self, *, carry, forward_state):
        carry, forward_state = self.repeat_layer(
            carry=carry, forward_state=forward_state["repeat_layer"]
        )
        return carry, dict(repeat_layer=forward_state)


class RepeatTest(TestCase):
    """Tests repeat layer."""

    @parameterized.product(
        dtype=(jnp.float32, jnp.bfloat16),
        remat_spec=(None, RematSpec(prevent_cse=False)),
        drop_output=(None, config_for_function(_drop_by_regex).set(rules=["module_outputs.*"])),
    )
    def test_repeat(self, dtype, remat_spec, drop_output):
        batch_size, num_layers = 14, 4
        cfg = TestEnsemble.default_config().set(name="test", num_layers=num_layers, dtype=dtype)
        cfg.repeat_layer.set(remat_spec=remat_spec, drop_output=drop_output)
        layer: TestEnsemble = cfg.instantiate(parent=None)
        self.assertEqual(
            PartitionSpec(None),
            layer.create_parameter_specs_recursively()["repeat_layer"]["layer"]["inc"].mesh_axes,
        )
        layer_params = layer.initialize_parameters_recursively(prng_key=jax.random.PRNGKey(1))
        logging.info("layer params=%s", layer_params)

        input_forward_state = layer.init_forward_state(batch_size)
        (carry, output_forward_state), output_collection = F(
            layer,
            prng_key=jax.random.PRNGKey(2),
            state=layer_params,
            inputs=dict(
                carry=jnp.arange(batch_size, dtype=dtype),
                forward_state=input_forward_state,
            ),
            is_training=True,
            drop_output_collections=(),
        )
        logging.info("forward_state=%s", output_forward_state)
        logging.info("output_collection=%s", output_collection)
        assert_allclose(carry, jnp.arange(num_layers, num_layers + batch_size, dtype=dtype))
        self.assertEqual(shapes(input_forward_state), shapes(output_forward_state))
        assert_allclose(
            output_forward_state["repeat_layer"]["layer"],
            jnp.reshape(
                jnp.arange(batch_size)[None, :] + jnp.arange(num_layers, dtype=dtype)[:, None],
                (num_layers, batch_size),
            ),
        )
        # Check output collection.
        if drop_output is not None:
            self.assertEqual(
                get_recursively(output_collection.module_outputs, "repeat_layer/layer"),
                {},
            )
        else:
            assert_allclose(
                get_recursively(output_forward_state, "repeat_layer/layer"),
                get_recursively(output_collection.module_outputs, "repeat_layer/layer/state"),
            )
        # Check summaries.
        self.assertEqual(
            {"repeat_layer": {"layer": {"carry_mean": (num_layers,)}}},
            shapes(output_collection.summaries),
        )
        assert_allclose(
            0.5 * (batch_size - 1) + jnp.arange(num_layers, dtype=dtype),
            output_collection.summaries["repeat_layer"]["layer"]["carry_mean"],
        )
        if remat_spec is None:
            # pylint: disable-next=protected-access
            self.assertEmpty(layer.repeat_layer._remat_methods)
        else:
            # pylint: disable-next=protected-access
            self.assertSequenceEqual(layer.repeat_layer._remat_methods, ["forward"])

    @parameterized.parameters(
        itertools.product((jnp.float32, jnp.bfloat16), (None, RematSpec(prevent_cse=False)))
    )
    def test_repeat_prebuilt_forward(self, dtype, remat_spec):
        # Testing using modified parameters for feed-forwarding.
        for multiple_values in range(2):
            batch_size, num_layers = 14, 4
            cfg = TestEnsemble.default_config().set(name="test", num_layers=num_layers, dtype=dtype)
            cfg.repeat_layer.remat_spec = remat_spec
            cfg.repeat_layer.layer = TestComplicatedLayer.default_config()
            layer: TestEnsemble = cfg.instantiate(parent=None)
            repeat_layer_prebuilt = VDict(
                {
                    "layer": {
                        "layer1": {"inc": jnp.zeros(4, dtype=dtype)},
                        "layer2": {"inc": jnp.ones(4, dtype=dtype) * multiple_values},
                    }
                }
            )
            prebuilt = {"dummy_layer": {"inc": None}, "repeat_layer": repeat_layer_prebuilt}
            layer_params_repeated_prebuilt = layer.initialize_parameters_recursively(
                prng_key=jax.random.PRNGKey(1), prebuilt=prebuilt
            )
            input_forward_state = layer.init_forward_state(batch_size)
            (carry, output_forward_state), output_collection = F(
                layer,
                prng_key=jax.random.PRNGKey(2),
                state=layer_params_repeated_prebuilt,
                inputs=dict(
                    carry=jnp.arange(batch_size, dtype=dtype),
                    forward_state=input_forward_state,
                ),
                is_training=True,
            )
            assert_allclose(
                carry,
                jnp.arange(
                    num_layers * multiple_values,
                    num_layers * multiple_values + batch_size,
                    dtype=dtype,
                ),
            )
            assert_allclose(
                output_forward_state["repeat_layer"]["layer"],
                jnp.reshape(
                    (
                        jnp.arange(batch_size)[None, :]
                        + jnp.arange(num_layers, dtype=dtype)[:, None] * multiple_values
                    )
                    * 2,
                    (num_layers, batch_size),
                ),
            )
            assert_allclose(
                0.5 * (batch_size - 1) + jnp.arange(num_layers, dtype=dtype) * multiple_values,
                output_collection.summaries["repeat_layer"]["layer"]["layer1"]["carry_mean"],
            )
            if remat_spec is None:
                # pylint: disable-next=protected-access
                self.assertEmpty(layer.repeat_layer._remat_methods)
            else:
                # pylint: disable-next=protected-access
                self.assertSequenceEqual(layer.repeat_layer._remat_methods, ["forward"])

    @parameterized.product(
        dtype=[jnp.float32, jnp.bfloat16],
        remat_spec=[None, RematSpec(prevent_cse=False)],
    )
    def test_shared_module(self, dtype, remat_spec):
        """Test repeat with shared modules."""
        batch_size, num_layers = 14, 4

        cfg = ParentLayer.default_config().set(
            shared_modules=["shared_layer"],
            children=dict(
                shared_layer=TestLayer.default_config().set(remat_spec=remat_spec, dtype=dtype),
                # Repeat to a shared module.
                repeat=TestRepeat.default_config().set(
                    layer=RedirectToSharedModule.default_config().set(
                        shared_module="shared_layer",
                        remat_spec=remat_spec,
                    ),
                    num_layers=num_layers,
                ),
                # Test nested repeat to a shared module.
                nested=ParentLayer.default_config().set(
                    children=dict(
                        repeat=TestRepeat.default_config().set(
                            layer=RedirectToSharedModule.default_config().set(
                                shared_module="shared_layer",
                                remat_spec=remat_spec,
                            ),
                            num_layers=num_layers,
                        ),
                    ),
                ),
            ),
        )
        layer = cfg.set(name="test").instantiate(parent=None)
        layer_params = layer.initialize_parameters_recursively(prng_key=jax.random.PRNGKey(1))
        logging.info("layer params=%s", layer_params)

        input_forward_state = layer.shared_layer.init_forward_state(batch_size)
        input_forward_state = dict(
            layer=jax.tree_util.tree_map(
                lambda x: jnp.tile(x[None, :], [num_layers, 1]),
                input_forward_state,
            )
        )

        for forward_path, output_path, remat_methods in [
            ("repeat", "repeat/layer/redirect/carry_mean", ["forward"]),
            ("nested/repeat", "nested/repeat/layer/redirect/carry_mean", ["forward", "forward"]),
        ]:
            (carry, output_forward_state), output_collection = F(
                layer,
                prng_key=jax.random.PRNGKey(2),
                state=layer_params,
                inputs=dict(
                    carry=jnp.arange(batch_size, dtype=dtype),
                    forward_state=input_forward_state,
                    path=tuple(forward_path.split("/")),
                ),
                is_training=True,
            )
            assert_allclose(carry, jnp.arange(num_layers, num_layers + batch_size, dtype=dtype))
            self.assertEqual(shapes(input_forward_state), shapes(output_forward_state))
            assert_allclose(
                output_forward_state["layer"],
                jnp.reshape(
                    jnp.arange(batch_size)[None, :] + jnp.arange(num_layers, dtype=dtype)[:, None],
                    (num_layers, batch_size),
                ),
            )

            # Construct expected output.
            expected_shapes = curr = {}
            for path in output_path.split("/")[:-1]:
                curr[path] = curr.get(path, {})
                curr = curr[path]
            curr[output_path.rsplit("/", maxsplit=1)[-1]] = (num_layers,)

            # Compare.
            self.assertEqual(expected_shapes, shapes(output_collection.summaries))
            assert_allclose(
                0.5 * (batch_size - 1) + jnp.arange(num_layers, dtype=dtype),
                get_recursively(output_collection.summaries, output_path.split("/")),
            )

            # Test remat spec.
            if remat_spec is None:
                # pylint: disable-next=protected-access
                self.assertEmpty(layer.shared_layer._remat_methods)
            else:
                # pylint: disable-next=protected-access
                self.assertSequenceEqual(layer.shared_layer._remat_methods, remat_methods)


if __name__ == "__main__":
    absltest.main()
