# Copyright Â© 2023 Apple Inc.

"""GPT models on C4.

    # Example using fake configs/data (commonly for testing).
    mkdir -p /tmp/gpt_c4_test;
    python3 -m axlearn.common.launch_trainer_main \
        --module=text.gpt.c4_trainer --config=fuji-test-v1 \
        --trainer_dir=/tmp/gpt_c4_test --data_dir=FAKE --jax_backend=cpu \
        --status_port=7337

    # Example training Fuji-7B with C4 dataset (can run on a single H100).
    XLA_FLAGS=--xla_dump_to=/tmp/xla_dump;
    mkdir -p /tmp/gpt_c4_test; \
    python3 -m axlearn.common.launch_trainer_main \
    --module=text.gpt.c4_trainer --config=fuji-7B-v2-single-host \
    --trainer_dir=/tmp/gpt_c4_test --data_dir=gs://axlearn-public/tensorflow_datasets \
    --jax_backend=gpu

    GS_ROOT=gs://my-bucket; \
    CONFIG=fuji-7B-v2; \
    INSTANCE_TYPE=tpu-v4-1024; \
    NUM_TPU_SLICES=1; \
    EXP=$(echo "text-gpt-c4-${CONFIG}-$(date +%F-%H%M)" | tr '[:upper:]' '[:lower:]'); \
    OUTPUT_DIR=$GS_ROOT/$USER/experiments/$EXP; \
    axlearn gcp launch --zone=$ZONE --instance_type=$INSTANCE_TYPE --num_slices=${NUM_TPU_SLICES} \
        --output_dir=$OUTPUT_DIR --name=$USER-$EXP -- \
        python3 -m axlearn.common.launch_trainer_main \
        --module=text.gpt.c4_trainer --config=$CONFIG \
        --trainer_dir=$OUTPUT_DIR \
        --data_dir=$GS_ROOT/tensorflow_datasets \
        --mesh_selector=$INSTANCE_TYPE --jax_backend=tpu

"""

from typing import Dict

from axlearn.common.config import InstantiableConfig, config_for_function
from axlearn.common.input_lm import lm_text_preprocessor
from axlearn.common.utils import get_data_dir
from axlearn.experiments.text.common import DataMixtureComponent, vocab
from axlearn.experiments.text.gpt import fuji, gspmd
from axlearn.experiments.text.gpt.common import mixture_train_input_source, tfds_input
from axlearn.experiments.trainer_config_utils import TrainerConfigFn

# Sentencepiece vocabs generated from c4/en:3.0.1.
# See bpe_{32k,128k}.json for the sentencepiece settings.
_SENTENCEPIECE_MODEL_NAME = {
    32 * 1024: "bpe_32k_c4.model",
    128 * 1024: "bpe_128k_c4.model",  # TODO(ruoming): build the 128k vocab.
}
_train_data_mixture_components = [
    DataMixtureComponent(
        name="c4/en:3.0.1",
        split="train",
        shuffle_buffer_size=8192,
        weight=1.0,
    ),
]


def _eval_input_sources(
    *, vocab_size: int, max_sequence_length: int
) -> Dict[str, InstantiableConfig]:
    return {
        name: config_for_function(tfds_input).set(
            dataset_name="c4/en:3.0.1",
            split=split,
            is_training=False,
            vocab_cfg=config_for_function(vocab).set(
                sentencepiece_model_name=_SENTENCEPIECE_MODEL_NAME[vocab_size]
            ),
            max_sequence_length=max_sequence_length,
        )
        for name, split in (("train", "train[:8192]"), ("validation", "validation"))
    }


def _train_input_source(*, vocab_size: int, max_sequence_length: int) -> InstantiableConfig:
    source_cfg = config_for_function(mixture_train_input_source).set(
        data_mixture_components=_train_data_mixture_components,
        vocab_cfg=config_for_function(vocab).set(
            sentencepiece_model_name=_SENTENCEPIECE_MODEL_NAME[vocab_size]
        ),
        max_sequence_length=max_sequence_length,
        preprocessor=config_for_function(lm_text_preprocessor).set(max_padding_fraction=0.5),
    )
    if get_data_dir() == "FAKE":
        source_cfg.preprocessor.shuffle_buffer_size = 0
    return source_cfg


def named_trainer_configs() -> Dict[str, TrainerConfigFn]:
    """Returns a mapping from trainer config names to TrainerConfigFn's."""
    config_map = {}
    config_map.update(fuji.trainer_configs(_train_input_source, _eval_input_sources))
    config_map.update(gspmd.trainer_configs(_train_input_source, _eval_input_sources))
    return config_map
